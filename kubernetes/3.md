# Replica Sets

- mention in desired state about how many replicas you need.
- Labels
- Selectors
    - help in identifying objects in K8s cluster via lables. 
    - equality/set based selection, eg: in, not in
    - selector is something, based on which the replica sets manages.
    - however, pods can have different labels
- logs of dying pods are kept for some time, the sidecar container keeps sending the logs to splunk/etc.
`kubectl scale rs <replicaset name> replica=10`
apply the yaml as a patch
`kubectl apply -f web-app-rs.yaml`
- patch changes have to be made in replicaset spec only
- in case of a crash, delete the replicaset, then create/apply, applying on a crashed container will not do anything.

## container manifests for container health checks
- livenessProbe
    - is the container running?
    - are there any memory issues, things that make a container.
    - http, shell command, tcp
    `livenessProbe:
        httpGet:
            path: /health
            port: 8080
        initialDelaySeconds: 5
        periodSeconds: 3`
- readiness Probe
    - is the container ready to serve the application?
    - check for things that make an application.


# Services
- load balances pods
- ip, DNS, Port => these are persistant
- kube-proxy has information about services, in the iptables
- Type = ClusterIP means this is accessible only from within the cluster
    - NodePort type , ip becomes accessible from outside the cluster
- endpoints, is the list of pods, this is updated in case of pod dying and restarting
- how to access our service outside of cluster
- relying on NodePort is not cool, as it may go down.
    - Type = LoadBalancer
    - Cloud provider should support this, GKE already does this by default, in aws you might want to go via elasticip/dns etc.?
- internally, pods are available through service names directly instead of pod IP

# Deployments
- 0 downtime
- rollbacks


* change context by `kubectl use-context`, connect to any other k8s cluster

# Ingress Controller, Ingress resource
- GKE will automatically create an ALB(application load balancer)
- on ly local i need a Ingress Controller (which will run nginx), this will be used as an ALB
- kind: Ingress, this is the configuration that goes in nginx config,
- but you need nginx also running, that is another POD, that runs nginx
- annotations:
    kubernetes.io/ingress.class: "nginx"



# Deployments

- what is missing in Replica Sets?
- updates with zero downtime
- rollbacks

- abstraction on top of replica sets
- provide updates and rollbacks for replica sets
- if you are doing rolling update, your application should be backward compatible.
    - version1, version2 serving together may lead to inconsistency, you have to manage it.

# Storage & Volumes
- emptyDir => in pod, pod containers can access
- hostPath => in node, containers of pods in the node can access
- awsElasticBlockStore => in cloud, containers of pods in all nodes can access

curl --header "Content-Type: application/json" \
  --request POST \
  --data '{"group":"suyash","name":"city","value":"Pune"}' \
  http://192.168.99.100:32308/metadata

# Persistent Volumes (PV), Persistent Volume Claims(PVC)
- pod, has a volume mount, volume names
- volume names point to PVC
- PVC keeps a collection of PV pointers
- if the pod goes down, it comes up and connects to the same PVC, which kept the collection of PV pointers
- Static/Dynamic PVs
- StorageClass => written in PVC, which specifies if i need a dynamic PV to be attached
- Access Modes, volume to node
    - ReadWriteOnce -> prefer
    - ReadOnlyMany
    - ReadWriteMany


# Networking
- every pod has a unique IP
- nodes have their own ip, there is a pod cidr in every node
- node to pod ip is there in route table(root netns)
- pod - pod talks via kube proxy ==> within same node, uses root netns
- node - node via route table

- 